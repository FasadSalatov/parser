#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä –∑–∞–∫–∞–∑–æ–≤ —Å FL.ru
–ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å –ø–µ—Ä–µ—Ö–æ–¥–æ–º –ø–æ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º —Å—Å—ã–ª–∫–∞–º
"""

import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime
from typing import List, Dict, Set
import logging

class FLParser:
    def __init__(self, user_agent: str, cookies: dict = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ FL.ru"""
        self.user_agent = user_agent
        self.cookies = cookies or {}
        self.sent_orders: Set[str] = set()  # –•—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –≤ –ø–∞–º—è—Ç–∏
        self.base_url = "https://www.fl.ru"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        if self.cookies:
            self.session.cookies.update(self.cookies)
    
    def get_new_orders(self, pages: int = 3) -> List[Dict]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–∫–∞–∑–æ–≤ —Å FL.ru (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –∑–∞–∫–∞–∑–æ–≤)
        """
        self.logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ FL.ru (–ø–µ—Ä–≤—ã–µ 5 –∑–∞–∫–∞–∑–æ–≤)...")
        all_orders = []
        
        # –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 5 –∑–∞–∫–∞–∑–∞–º–∏
        try:
            page_orders = self._parse_page(1, max_orders=5)
            all_orders.extend(page_orders)
            self.logger.info(f"üìÑ –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ FL.ru: –Ω–∞–π–¥–µ–Ω–æ {len(page_orders)} –∑–∞–∫–∞–∑–æ–≤ (–ª–∏–º–∏—Ç: 5)")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ FL.ru: {str(e)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–∞–∫–∞–∑—ã
        new_orders = []
        for order in all_orders:
            order_id = order.get('id')
            if order_id and order_id not in self.sent_orders:
                new_orders.append(order)
                self.sent_orders.add(order_id)
        
        self.logger.info(f"‚úÖ FL.ru: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_orders)} –∑–∞–∫–∞–∑–æ–≤, –Ω–æ–≤—ã—Ö: {len(new_orders)}")
        return new_orders
    
    def _parse_page(self, page: int, max_orders: int = None) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–∫–∞–∑–æ–≤"""
        url = f"{self.base_url}/projects/category/programmirovanie/"
        if page > 1:
            url += f"?page={page}"
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            orders = []
            
            # –ò—â–µ–º –±–ª–æ–∫–∏ —Å –∑–∞–∫–∞–∑–∞–º–∏ - h2 —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –ø—Ä–æ–µ–∫—Ç—ã
            h2_elements = soup.find_all('h2')
            
            for block in h2_elements:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –∑–∞–∫–∞–∑–æ–≤
                    if max_orders and len(orders) >= max_orders:
                        break
                    
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤ h2
                    link = block.find('a')
                    if not link:
                        continue
                    
                    href = link.get('href', '')
                    if not href or '/projects/' not in href or 'category' in href:
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞–∫–∞–∑–∞ –∏–∑ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    order = self._extract_order_from_individual_page(href)
                    if order:
                        orders.append(order)
                        
                except Exception as e:
                    self.logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–ª–æ–∫–∞: {str(e)}")
                    continue
            
            return orders
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {str(e)}")
            return []
    
    def _extract_order_from_individual_page(self, href: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–∞ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
            if href.startswith('/'):
                full_url = f"{self.base_url}{href}"
            else:
                full_url = href
            
            # ID –∑–∞–∫–∞–∑–∞ –∏–∑ URL
            order_id_match = re.search(r'/projects/(\d+)/', href)
            if not order_id_match:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö—ç—à –æ—Ç URL –∫–∞–∫ ID
                order_id = f"fl_{abs(hash(href)) % 1000000}"
            else:
                order_id = f"fl_{order_id_match.group(1)}"
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∑–∞–∫–∞–∑–∞
            response = self.session.get(full_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            order_data = self._parse_individual_page(soup, full_url, order_id)
            
            return order_data
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã {href}: {str(e)}")
            return None
    
    def _parse_individual_page(self, soup: BeautifulSoup, url: str, order_id: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–∫–∞–∑–∞"""
        try:
            # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –∏—â–µ–º h1 –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ title
            title = "–ó–∞–∫–∞–∑ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º h1
            h1_element = soup.find('h1')
            if h1_element:
                title = self._clean_text(h1_element.get_text())
            
            # –ï—Å–ª–∏ h1 –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –≤ title —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not title or title == "–ó–∞–∫–∞–∑ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è":
                title_element = soup.find('title')
                if title_element:
                    title_text = title_element.get_text()
                    # –£–±–∏—Ä–∞–µ–º "FL.ru" –∏ –¥—Ä—É–≥–∏–µ –ª–∏—à–Ω–∏–µ —á–∞—Å—Ç–∏
                    title = title_text.split(' | ')[0].split(' - ')[0].strip()
                    title = self._clean_text(title)
            
            # 2. –ë—é–¥–∂–µ—Ç - –∏—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            price = "–ü–æ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏"
            page_text = soup.get_text()
            
            budget_patterns = [
                r'–ë—é–¥–∂–µ—Ç:\s*([^\n\r]+)',
                r'–±—é–¥–∂–µ—Ç:\s*([^\n\r]+)',
                r'–ø–æ –¥–æ–≥–æ–≤–æ—Ä[–µ—ë]–Ω–Ω–æ—Å—Ç–∏',
                r'(\d+[\s\d,]*)\s*—Ä—É–±',
                r'(\d+[\s\d,]*)\s*—Ä—É–±–ª–µ–π'
            ]
            
            for pattern in budget_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    if '–¥–æ–≥–æ–≤–æ—Ä' in pattern:
                        price = "–ü–æ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏"
                    else:
                        price = match.group(1).strip()
                    break
            
            # 3. –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–∫–∞–∑–∞ - –±–µ—Ä—ë–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–ª–∏ –∏—â–µ–º –±–æ–ª—å—à–æ–π –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞
            description = title
            
            # –ò—â–µ–º div —Å —Ç–µ–∫—Å—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ–º
            desc_candidates = soup.find_all(['div', 'p'], string=lambda text: text and len(text.strip()) > 30)
            for candidate in desc_candidates:
                text = candidate.get_text().strip()
                if len(text) > 50 and text != title:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ —Ñ—É—Ç–µ—Ä –∏–ª–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ç–µ–∫—Å—Ç
                    if not any(word in text.lower() for word in ['—Å–≤–µ–¥–µ–Ω–∏—è –æ–± –æ–æ–æ', 'fl.ru', 'copyright', '¬©']):
                        description = text[:300]
                        break
            
            # 4. –ê–≤—Ç–æ—Ä - –∏—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–∫–∞–∑—á–∏–∫–µ
            author = "–ù–µ —É–∫–∞–∑–∞–Ω"
            
            # –ò—â–µ–º –∑–∞–∫–∞–∑—á–∏–∫–∞
            customer_patterns = [
                r'–ó–∞–∫–∞–∑—á–∏–∫\s*([^\n\r]+)',
                r'–∑–∞–∫–∞–∑—á–∏–∫\s*([^\n\r]+)',
                r'–ê–≤—Ç–æ—Ä:\s*([^\n\r]+)',
                r'–∞–≤—Ç–æ—Ä:\s*([^\n\r]+)'
            ]
            
            for pattern in customer_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    author = match.group(1).strip()[:50]
                    break
            
            # 5. –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ - –∏—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            published = "–ù–µ–¥–∞–≤–Ω–æ"
            
            date_patterns = [
                r'–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω:\s*([^\n\r]+)',
                r'–æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω:\s*([^\n\r]+)',
                r'–†–∞–∑–º–µ—â–µ–Ω:\s*([^\n\r]+)',
                r'—Ä–∞–∑–º–µ—â–µ–Ω:\s*([^\n\r]+)',
                r'(\d{1,2}[./]\d{1,2}[./]\d{4})',
                r'(\d+)\s*–º–∏–Ω—É—Ç[—É—ã]?\s*–Ω–∞–∑–∞–¥',
                r'(\d+)\s*—á–∞—Å[–æ–≤–∞]?\s*–Ω–∞–∑–∞–¥',
                r'(\d+)\s*–¥–Ω[—è–µ–π]+\s*–Ω–∞–∑–∞–¥'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    published = match.group(1).strip() if '–æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω' in pattern.lower() else match.group(0)
                    break
            
            order = {
                'id': order_id,
                'title': title,
                'url': url,
                'source': 'FL.ru',
                'price': price,
                'category': '–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ',
                'author': author,
                'published': published,
                'description': description,
                'parsed_at': datetime.now().isoformat()
            }
            
            return order
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)}")
            return None
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = text.replace('\n', ' ').replace('\t', ' ')
        
        return text[:200]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É 